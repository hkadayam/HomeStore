// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_HOMESTORECONFIG_HOMESTORECFG_H_
#define FLATBUFFERS_GENERATED_HOMESTORECONFIG_HOMESTORECFG_H_

#include "flatbuffers/flatbuffers.h"

#include "utility/non_null_ptr.hpp"

namespace homestorecfg {

struct BlkAllocator;
struct BlkAllocatorT;

struct Btree;
struct BtreeT;

struct Cache;
struct CacheT;

struct Device;
struct DeviceT;

struct Generic;
struct GenericT;

struct HomeStoreSettings;
struct HomeStoreSettingsT;

inline const flatbuffers::TypeTable *BlkAllocatorTypeTable();

inline const flatbuffers::TypeTable *BtreeTypeTable();

inline const flatbuffers::TypeTable *CacheTypeTable();

inline const flatbuffers::TypeTable *DeviceTypeTable();

inline const flatbuffers::TypeTable *GenericTypeTable();

inline const flatbuffers::TypeTable *HomeStoreSettingsTypeTable();

struct BlkAllocatorT : public flatbuffers::NativeTable {
  typedef BlkAllocator TableType;
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.BlkAllocatorT";
  }
  uint32_t max_cache_fill_varsize_blk_alloc_attempt;
  uint32_t max_varsize_blk_alloc_attempt;
  BlkAllocatorT()
      : max_cache_fill_varsize_blk_alloc_attempt(3),
        max_varsize_blk_alloc_attempt(1000) {
  }
};

struct BlkAllocator FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef BlkAllocatorT NativeTableType;
  static const flatbuffers::TypeTable *MiniReflectTypeTable() {
    return BlkAllocatorTypeTable();
  }
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.BlkAllocator";
  }
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_CACHE_FILL_VARSIZE_BLK_ALLOC_ATTEMPT = 4,
    VT_MAX_VARSIZE_BLK_ALLOC_ATTEMPT = 6
  };
  uint32_t max_cache_fill_varsize_blk_alloc_attempt() const {
    return GetField<uint32_t>(VT_MAX_CACHE_FILL_VARSIZE_BLK_ALLOC_ATTEMPT, 3);
  }
  bool mutate_max_cache_fill_varsize_blk_alloc_attempt(uint32_t _max_cache_fill_varsize_blk_alloc_attempt) {
    return SetField<uint32_t>(VT_MAX_CACHE_FILL_VARSIZE_BLK_ALLOC_ATTEMPT, _max_cache_fill_varsize_blk_alloc_attempt, 3);
  }
  uint32_t max_varsize_blk_alloc_attempt() const {
    return GetField<uint32_t>(VT_MAX_VARSIZE_BLK_ALLOC_ATTEMPT, 1000);
  }
  bool mutate_max_varsize_blk_alloc_attempt(uint32_t _max_varsize_blk_alloc_attempt) {
    return SetField<uint32_t>(VT_MAX_VARSIZE_BLK_ALLOC_ATTEMPT, _max_varsize_blk_alloc_attempt, 1000);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_MAX_CACHE_FILL_VARSIZE_BLK_ALLOC_ATTEMPT) &&
           VerifyField<uint32_t>(verifier, VT_MAX_VARSIZE_BLK_ALLOC_ATTEMPT) &&
           verifier.EndTable();
  }
  BlkAllocatorT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(BlkAllocatorT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<BlkAllocator> Pack(flatbuffers::FlatBufferBuilder &_fbb, const BlkAllocatorT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct BlkAllocatorBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_max_cache_fill_varsize_blk_alloc_attempt(uint32_t max_cache_fill_varsize_blk_alloc_attempt) {
    fbb_.AddElement<uint32_t>(BlkAllocator::VT_MAX_CACHE_FILL_VARSIZE_BLK_ALLOC_ATTEMPT, max_cache_fill_varsize_blk_alloc_attempt, 3);
  }
  void add_max_varsize_blk_alloc_attempt(uint32_t max_varsize_blk_alloc_attempt) {
    fbb_.AddElement<uint32_t>(BlkAllocator::VT_MAX_VARSIZE_BLK_ALLOC_ATTEMPT, max_varsize_blk_alloc_attempt, 1000);
  }
  explicit BlkAllocatorBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  BlkAllocatorBuilder &operator=(const BlkAllocatorBuilder &);
  flatbuffers::Offset<BlkAllocator> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<BlkAllocator>(end);
    return o;
  }
};

inline flatbuffers::Offset<BlkAllocator> CreateBlkAllocator(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t max_cache_fill_varsize_blk_alloc_attempt = 3,
    uint32_t max_varsize_blk_alloc_attempt = 1000) {
  BlkAllocatorBuilder builder_(_fbb);
  builder_.add_max_varsize_blk_alloc_attempt(max_varsize_blk_alloc_attempt);
  builder_.add_max_cache_fill_varsize_blk_alloc_attempt(max_cache_fill_varsize_blk_alloc_attempt);
  return builder_.Finish();
}

flatbuffers::Offset<BlkAllocator> CreateBlkAllocator(flatbuffers::FlatBufferBuilder &_fbb, const BlkAllocatorT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct BtreeT : public flatbuffers::NativeTable {
  typedef Btree TableType;
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.BtreeT";
  }
  uint32_t max_nodes_to_rebalance;
  uint32_t mem_btree_page_size;
  BtreeT()
      : max_nodes_to_rebalance(3),
        mem_btree_page_size(8192) {
  }
};

struct Btree FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef BtreeT NativeTableType;
  static const flatbuffers::TypeTable *MiniReflectTypeTable() {
    return BtreeTypeTable();
  }
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.Btree";
  }
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_NODES_TO_REBALANCE = 4,
    VT_MEM_BTREE_PAGE_SIZE = 6
  };
  uint32_t max_nodes_to_rebalance() const {
    return GetField<uint32_t>(VT_MAX_NODES_TO_REBALANCE, 3);
  }
  bool mutate_max_nodes_to_rebalance(uint32_t _max_nodes_to_rebalance) {
    return SetField<uint32_t>(VT_MAX_NODES_TO_REBALANCE, _max_nodes_to_rebalance, 3);
  }
  uint32_t mem_btree_page_size() const {
    return GetField<uint32_t>(VT_MEM_BTREE_PAGE_SIZE, 8192);
  }
  bool mutate_mem_btree_page_size(uint32_t _mem_btree_page_size) {
    return SetField<uint32_t>(VT_MEM_BTREE_PAGE_SIZE, _mem_btree_page_size, 8192);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_MAX_NODES_TO_REBALANCE) &&
           VerifyField<uint32_t>(verifier, VT_MEM_BTREE_PAGE_SIZE) &&
           verifier.EndTable();
  }
  BtreeT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(BtreeT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Btree> Pack(flatbuffers::FlatBufferBuilder &_fbb, const BtreeT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct BtreeBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_max_nodes_to_rebalance(uint32_t max_nodes_to_rebalance) {
    fbb_.AddElement<uint32_t>(Btree::VT_MAX_NODES_TO_REBALANCE, max_nodes_to_rebalance, 3);
  }
  void add_mem_btree_page_size(uint32_t mem_btree_page_size) {
    fbb_.AddElement<uint32_t>(Btree::VT_MEM_BTREE_PAGE_SIZE, mem_btree_page_size, 8192);
  }
  explicit BtreeBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  BtreeBuilder &operator=(const BtreeBuilder &);
  flatbuffers::Offset<Btree> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Btree>(end);
    return o;
  }
};

inline flatbuffers::Offset<Btree> CreateBtree(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t max_nodes_to_rebalance = 3,
    uint32_t mem_btree_page_size = 8192) {
  BtreeBuilder builder_(_fbb);
  builder_.add_mem_btree_page_size(mem_btree_page_size);
  builder_.add_max_nodes_to_rebalance(max_nodes_to_rebalance);
  return builder_.Finish();
}

flatbuffers::Offset<Btree> CreateBtree(flatbuffers::FlatBufferBuilder &_fbb, const BtreeT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct CacheT : public flatbuffers::NativeTable {
  typedef Cache TableType;
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.CacheT";
  }
  uint32_t entries_per_hash_bucket;
  uint32_t num_evictor_partitions;
  CacheT()
      : entries_per_hash_bucket(2),
        num_evictor_partitions(32) {
  }
};

struct Cache FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef CacheT NativeTableType;
  static const flatbuffers::TypeTable *MiniReflectTypeTable() {
    return CacheTypeTable();
  }
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.Cache";
  }
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ENTRIES_PER_HASH_BUCKET = 4,
    VT_NUM_EVICTOR_PARTITIONS = 6
  };
  uint32_t entries_per_hash_bucket() const {
    return GetField<uint32_t>(VT_ENTRIES_PER_HASH_BUCKET, 2);
  }
  bool mutate_entries_per_hash_bucket(uint32_t _entries_per_hash_bucket) {
    return SetField<uint32_t>(VT_ENTRIES_PER_HASH_BUCKET, _entries_per_hash_bucket, 2);
  }
  uint32_t num_evictor_partitions() const {
    return GetField<uint32_t>(VT_NUM_EVICTOR_PARTITIONS, 32);
  }
  bool mutate_num_evictor_partitions(uint32_t _num_evictor_partitions) {
    return SetField<uint32_t>(VT_NUM_EVICTOR_PARTITIONS, _num_evictor_partitions, 32);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_ENTRIES_PER_HASH_BUCKET) &&
           VerifyField<uint32_t>(verifier, VT_NUM_EVICTOR_PARTITIONS) &&
           verifier.EndTable();
  }
  CacheT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(CacheT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Cache> Pack(flatbuffers::FlatBufferBuilder &_fbb, const CacheT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct CacheBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_entries_per_hash_bucket(uint32_t entries_per_hash_bucket) {
    fbb_.AddElement<uint32_t>(Cache::VT_ENTRIES_PER_HASH_BUCKET, entries_per_hash_bucket, 2);
  }
  void add_num_evictor_partitions(uint32_t num_evictor_partitions) {
    fbb_.AddElement<uint32_t>(Cache::VT_NUM_EVICTOR_PARTITIONS, num_evictor_partitions, 32);
  }
  explicit CacheBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  CacheBuilder &operator=(const CacheBuilder &);
  flatbuffers::Offset<Cache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Cache>(end);
    return o;
  }
};

inline flatbuffers::Offset<Cache> CreateCache(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t entries_per_hash_bucket = 2,
    uint32_t num_evictor_partitions = 32) {
  CacheBuilder builder_(_fbb);
  builder_.add_num_evictor_partitions(num_evictor_partitions);
  builder_.add_entries_per_hash_bucket(entries_per_hash_bucket);
  return builder_.Finish();
}

flatbuffers::Offset<Cache> CreateCache(flatbuffers::FlatBufferBuilder &_fbb, const CacheT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct DeviceT : public flatbuffers::NativeTable {
  typedef Device TableType;
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.DeviceT";
  }
  uint32_t max_error_before_marking_dev_down;
  uint32_t max_outstanding_ios_per_aio_thread;
  uint32_t max_completions_process_per_event_per_thread;
  DeviceT()
      : max_error_before_marking_dev_down(100),
        max_outstanding_ios_per_aio_thread(200),
        max_completions_process_per_event_per_thread(200) {
  }
};

struct Device FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef DeviceT NativeTableType;
  static const flatbuffers::TypeTable *MiniReflectTypeTable() {
    return DeviceTypeTable();
  }
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.Device";
  }
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_ERROR_BEFORE_MARKING_DEV_DOWN = 4,
    VT_MAX_OUTSTANDING_IOS_PER_AIO_THREAD = 6,
    VT_MAX_COMPLETIONS_PROCESS_PER_EVENT_PER_THREAD = 8
  };
  uint32_t max_error_before_marking_dev_down() const {
    return GetField<uint32_t>(VT_MAX_ERROR_BEFORE_MARKING_DEV_DOWN, 100);
  }
  bool mutate_max_error_before_marking_dev_down(uint32_t _max_error_before_marking_dev_down) {
    return SetField<uint32_t>(VT_MAX_ERROR_BEFORE_MARKING_DEV_DOWN, _max_error_before_marking_dev_down, 100);
  }
  uint32_t max_outstanding_ios_per_aio_thread() const {
    return GetField<uint32_t>(VT_MAX_OUTSTANDING_IOS_PER_AIO_THREAD, 200);
  }
  bool mutate_max_outstanding_ios_per_aio_thread(uint32_t _max_outstanding_ios_per_aio_thread) {
    return SetField<uint32_t>(VT_MAX_OUTSTANDING_IOS_PER_AIO_THREAD, _max_outstanding_ios_per_aio_thread, 200);
  }
  uint32_t max_completions_process_per_event_per_thread() const {
    return GetField<uint32_t>(VT_MAX_COMPLETIONS_PROCESS_PER_EVENT_PER_THREAD, 200);
  }
  bool mutate_max_completions_process_per_event_per_thread(uint32_t _max_completions_process_per_event_per_thread) {
    return SetField<uint32_t>(VT_MAX_COMPLETIONS_PROCESS_PER_EVENT_PER_THREAD, _max_completions_process_per_event_per_thread, 200);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_MAX_ERROR_BEFORE_MARKING_DEV_DOWN) &&
           VerifyField<uint32_t>(verifier, VT_MAX_OUTSTANDING_IOS_PER_AIO_THREAD) &&
           VerifyField<uint32_t>(verifier, VT_MAX_COMPLETIONS_PROCESS_PER_EVENT_PER_THREAD) &&
           verifier.EndTable();
  }
  DeviceT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(DeviceT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Device> Pack(flatbuffers::FlatBufferBuilder &_fbb, const DeviceT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct DeviceBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_max_error_before_marking_dev_down(uint32_t max_error_before_marking_dev_down) {
    fbb_.AddElement<uint32_t>(Device::VT_MAX_ERROR_BEFORE_MARKING_DEV_DOWN, max_error_before_marking_dev_down, 100);
  }
  void add_max_outstanding_ios_per_aio_thread(uint32_t max_outstanding_ios_per_aio_thread) {
    fbb_.AddElement<uint32_t>(Device::VT_MAX_OUTSTANDING_IOS_PER_AIO_THREAD, max_outstanding_ios_per_aio_thread, 200);
  }
  void add_max_completions_process_per_event_per_thread(uint32_t max_completions_process_per_event_per_thread) {
    fbb_.AddElement<uint32_t>(Device::VT_MAX_COMPLETIONS_PROCESS_PER_EVENT_PER_THREAD, max_completions_process_per_event_per_thread, 200);
  }
  explicit DeviceBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  DeviceBuilder &operator=(const DeviceBuilder &);
  flatbuffers::Offset<Device> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Device>(end);
    return o;
  }
};

inline flatbuffers::Offset<Device> CreateDevice(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t max_error_before_marking_dev_down = 100,
    uint32_t max_outstanding_ios_per_aio_thread = 200,
    uint32_t max_completions_process_per_event_per_thread = 200) {
  DeviceBuilder builder_(_fbb);
  builder_.add_max_completions_process_per_event_per_thread(max_completions_process_per_event_per_thread);
  builder_.add_max_outstanding_ios_per_aio_thread(max_outstanding_ios_per_aio_thread);
  builder_.add_max_error_before_marking_dev_down(max_error_before_marking_dev_down);
  return builder_.Finish();
}

flatbuffers::Offset<Device> CreateDevice(flatbuffers::FlatBufferBuilder &_fbb, const DeviceT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct GenericT : public flatbuffers::NativeTable {
  typedef Generic TableType;
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.GenericT";
  }
  uint32_t cache_size_percent;
  uint32_t soft_mem_release_threshold;
  uint32_t aggressive_mem_release_threshold;
  uint32_t mem_release_rate;
  GenericT()
      : cache_size_percent(65),
        soft_mem_release_threshold(85),
        aggressive_mem_release_threshold(95),
        mem_release_rate(8) {
  }
};

struct Generic FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef GenericT NativeTableType;
  static const flatbuffers::TypeTable *MiniReflectTypeTable() {
    return GenericTypeTable();
  }
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.Generic";
  }
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_CACHE_SIZE_PERCENT = 4,
    VT_SOFT_MEM_RELEASE_THRESHOLD = 6,
    VT_AGGRESSIVE_MEM_RELEASE_THRESHOLD = 8,
    VT_MEM_RELEASE_RATE = 10
  };
  uint32_t cache_size_percent() const {
    return GetField<uint32_t>(VT_CACHE_SIZE_PERCENT, 65);
  }
  bool mutate_cache_size_percent(uint32_t _cache_size_percent) {
    return SetField<uint32_t>(VT_CACHE_SIZE_PERCENT, _cache_size_percent, 65);
  }
  uint32_t soft_mem_release_threshold() const {
    return GetField<uint32_t>(VT_SOFT_MEM_RELEASE_THRESHOLD, 85);
  }
  bool mutate_soft_mem_release_threshold(uint32_t _soft_mem_release_threshold) {
    return SetField<uint32_t>(VT_SOFT_MEM_RELEASE_THRESHOLD, _soft_mem_release_threshold, 85);
  }
  uint32_t aggressive_mem_release_threshold() const {
    return GetField<uint32_t>(VT_AGGRESSIVE_MEM_RELEASE_THRESHOLD, 95);
  }
  bool mutate_aggressive_mem_release_threshold(uint32_t _aggressive_mem_release_threshold) {
    return SetField<uint32_t>(VT_AGGRESSIVE_MEM_RELEASE_THRESHOLD, _aggressive_mem_release_threshold, 95);
  }
  uint32_t mem_release_rate() const {
    return GetField<uint32_t>(VT_MEM_RELEASE_RATE, 8);
  }
  bool mutate_mem_release_rate(uint32_t _mem_release_rate) {
    return SetField<uint32_t>(VT_MEM_RELEASE_RATE, _mem_release_rate, 8);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_CACHE_SIZE_PERCENT) &&
           VerifyField<uint32_t>(verifier, VT_SOFT_MEM_RELEASE_THRESHOLD) &&
           VerifyField<uint32_t>(verifier, VT_AGGRESSIVE_MEM_RELEASE_THRESHOLD) &&
           VerifyField<uint32_t>(verifier, VT_MEM_RELEASE_RATE) &&
           verifier.EndTable();
  }
  GenericT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(GenericT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<Generic> Pack(flatbuffers::FlatBufferBuilder &_fbb, const GenericT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct GenericBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_cache_size_percent(uint32_t cache_size_percent) {
    fbb_.AddElement<uint32_t>(Generic::VT_CACHE_SIZE_PERCENT, cache_size_percent, 65);
  }
  void add_soft_mem_release_threshold(uint32_t soft_mem_release_threshold) {
    fbb_.AddElement<uint32_t>(Generic::VT_SOFT_MEM_RELEASE_THRESHOLD, soft_mem_release_threshold, 85);
  }
  void add_aggressive_mem_release_threshold(uint32_t aggressive_mem_release_threshold) {
    fbb_.AddElement<uint32_t>(Generic::VT_AGGRESSIVE_MEM_RELEASE_THRESHOLD, aggressive_mem_release_threshold, 95);
  }
  void add_mem_release_rate(uint32_t mem_release_rate) {
    fbb_.AddElement<uint32_t>(Generic::VT_MEM_RELEASE_RATE, mem_release_rate, 8);
  }
  explicit GenericBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  GenericBuilder &operator=(const GenericBuilder &);
  flatbuffers::Offset<Generic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<Generic>(end);
    return o;
  }
};

inline flatbuffers::Offset<Generic> CreateGeneric(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t cache_size_percent = 65,
    uint32_t soft_mem_release_threshold = 85,
    uint32_t aggressive_mem_release_threshold = 95,
    uint32_t mem_release_rate = 8) {
  GenericBuilder builder_(_fbb);
  builder_.add_mem_release_rate(mem_release_rate);
  builder_.add_aggressive_mem_release_threshold(aggressive_mem_release_threshold);
  builder_.add_soft_mem_release_threshold(soft_mem_release_threshold);
  builder_.add_cache_size_percent(cache_size_percent);
  return builder_.Finish();
}

flatbuffers::Offset<Generic> CreateGeneric(flatbuffers::FlatBufferBuilder &_fbb, const GenericT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct HomeStoreSettingsT : public flatbuffers::NativeTable {
  typedef HomeStoreSettings TableType;
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.HomeStoreSettingsT";
  }
  uint32_t version;
  sisl::embedded_t<GenericT> generic;
  sisl::embedded_t<BlkAllocatorT> blkallocator;
  sisl::embedded_t<CacheT> cache;
  sisl::embedded_t<BtreeT> btree;
  sisl::embedded_t<DeviceT> device;
  HomeStoreSettingsT()
      : version(0) {
  }
};

struct HomeStoreSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef HomeStoreSettingsT NativeTableType;
  static const flatbuffers::TypeTable *MiniReflectTypeTable() {
    return HomeStoreSettingsTypeTable();
  }
  static FLATBUFFERS_CONSTEXPR const char *GetFullyQualifiedName() {
    return "homestorecfg.HomeStoreSettings";
  }
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_VERSION = 4,
    VT_GENERIC = 6,
    VT_BLKALLOCATOR = 8,
    VT_CACHE = 10,
    VT_BTREE = 12,
    VT_DEVICE = 14
  };
  uint32_t version() const {
    return GetField<uint32_t>(VT_VERSION, 0);
  }
  bool mutate_version(uint32_t _version) {
    return SetField<uint32_t>(VT_VERSION, _version, 0);
  }
  const Generic *generic() const {
    return GetPointer<const Generic *>(VT_GENERIC);
  }
  Generic *mutable_generic() {
    return GetPointer<Generic *>(VT_GENERIC);
  }
  const BlkAllocator *blkallocator() const {
    return GetPointer<const BlkAllocator *>(VT_BLKALLOCATOR);
  }
  BlkAllocator *mutable_blkallocator() {
    return GetPointer<BlkAllocator *>(VT_BLKALLOCATOR);
  }
  const Cache *cache() const {
    return GetPointer<const Cache *>(VT_CACHE);
  }
  Cache *mutable_cache() {
    return GetPointer<Cache *>(VT_CACHE);
  }
  const Btree *btree() const {
    return GetPointer<const Btree *>(VT_BTREE);
  }
  Btree *mutable_btree() {
    return GetPointer<Btree *>(VT_BTREE);
  }
  const Device *device() const {
    return GetPointer<const Device *>(VT_DEVICE);
  }
  Device *mutable_device() {
    return GetPointer<Device *>(VT_DEVICE);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint32_t>(verifier, VT_VERSION) &&
           VerifyOffset(verifier, VT_GENERIC) &&
           verifier.VerifyTable(generic()) &&
           VerifyOffset(verifier, VT_BLKALLOCATOR) &&
           verifier.VerifyTable(blkallocator()) &&
           VerifyOffset(verifier, VT_CACHE) &&
           verifier.VerifyTable(cache()) &&
           VerifyOffset(verifier, VT_BTREE) &&
           verifier.VerifyTable(btree()) &&
           VerifyOffset(verifier, VT_DEVICE) &&
           verifier.VerifyTable(device()) &&
           verifier.EndTable();
  }
  HomeStoreSettingsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(HomeStoreSettingsT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<HomeStoreSettings> Pack(flatbuffers::FlatBufferBuilder &_fbb, const HomeStoreSettingsT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct HomeStoreSettingsBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_version(uint32_t version) {
    fbb_.AddElement<uint32_t>(HomeStoreSettings::VT_VERSION, version, 0);
  }
  void add_generic(flatbuffers::Offset<Generic> generic) {
    fbb_.AddOffset(HomeStoreSettings::VT_GENERIC, generic);
  }
  void add_blkallocator(flatbuffers::Offset<BlkAllocator> blkallocator) {
    fbb_.AddOffset(HomeStoreSettings::VT_BLKALLOCATOR, blkallocator);
  }
  void add_cache(flatbuffers::Offset<Cache> cache) {
    fbb_.AddOffset(HomeStoreSettings::VT_CACHE, cache);
  }
  void add_btree(flatbuffers::Offset<Btree> btree) {
    fbb_.AddOffset(HomeStoreSettings::VT_BTREE, btree);
  }
  void add_device(flatbuffers::Offset<Device> device) {
    fbb_.AddOffset(HomeStoreSettings::VT_DEVICE, device);
  }
  explicit HomeStoreSettingsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  HomeStoreSettingsBuilder &operator=(const HomeStoreSettingsBuilder &);
  flatbuffers::Offset<HomeStoreSettings> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<HomeStoreSettings>(end);
    return o;
  }
};

inline flatbuffers::Offset<HomeStoreSettings> CreateHomeStoreSettings(
    flatbuffers::FlatBufferBuilder &_fbb,
    uint32_t version = 0,
    flatbuffers::Offset<Generic> generic = 0,
    flatbuffers::Offset<BlkAllocator> blkallocator = 0,
    flatbuffers::Offset<Cache> cache = 0,
    flatbuffers::Offset<Btree> btree = 0,
    flatbuffers::Offset<Device> device = 0) {
  HomeStoreSettingsBuilder builder_(_fbb);
  builder_.add_device(device);
  builder_.add_btree(btree);
  builder_.add_cache(cache);
  builder_.add_blkallocator(blkallocator);
  builder_.add_generic(generic);
  builder_.add_version(version);
  return builder_.Finish();
}

flatbuffers::Offset<HomeStoreSettings> CreateHomeStoreSettings(flatbuffers::FlatBufferBuilder &_fbb, const HomeStoreSettingsT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

inline BlkAllocatorT *BlkAllocator::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new BlkAllocatorT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void BlkAllocator::UnPackTo(BlkAllocatorT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = max_cache_fill_varsize_blk_alloc_attempt(); _o->max_cache_fill_varsize_blk_alloc_attempt = _e; };
  { auto _e = max_varsize_blk_alloc_attempt(); _o->max_varsize_blk_alloc_attempt = _e; };
}

inline flatbuffers::Offset<BlkAllocator> BlkAllocator::Pack(flatbuffers::FlatBufferBuilder &_fbb, const BlkAllocatorT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateBlkAllocator(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<BlkAllocator> CreateBlkAllocator(flatbuffers::FlatBufferBuilder &_fbb, const BlkAllocatorT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const BlkAllocatorT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _max_cache_fill_varsize_blk_alloc_attempt = _o->max_cache_fill_varsize_blk_alloc_attempt;
  auto _max_varsize_blk_alloc_attempt = _o->max_varsize_blk_alloc_attempt;
  return homestorecfg::CreateBlkAllocator(
      _fbb,
      _max_cache_fill_varsize_blk_alloc_attempt,
      _max_varsize_blk_alloc_attempt);
}

inline BtreeT *Btree::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new BtreeT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Btree::UnPackTo(BtreeT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = max_nodes_to_rebalance(); _o->max_nodes_to_rebalance = _e; };
  { auto _e = mem_btree_page_size(); _o->mem_btree_page_size = _e; };
}

inline flatbuffers::Offset<Btree> Btree::Pack(flatbuffers::FlatBufferBuilder &_fbb, const BtreeT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateBtree(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Btree> CreateBtree(flatbuffers::FlatBufferBuilder &_fbb, const BtreeT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const BtreeT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _max_nodes_to_rebalance = _o->max_nodes_to_rebalance;
  auto _mem_btree_page_size = _o->mem_btree_page_size;
  return homestorecfg::CreateBtree(
      _fbb,
      _max_nodes_to_rebalance,
      _mem_btree_page_size);
}

inline CacheT *Cache::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new CacheT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Cache::UnPackTo(CacheT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = entries_per_hash_bucket(); _o->entries_per_hash_bucket = _e; };
  { auto _e = num_evictor_partitions(); _o->num_evictor_partitions = _e; };
}

inline flatbuffers::Offset<Cache> Cache::Pack(flatbuffers::FlatBufferBuilder &_fbb, const CacheT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateCache(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Cache> CreateCache(flatbuffers::FlatBufferBuilder &_fbb, const CacheT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const CacheT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _entries_per_hash_bucket = _o->entries_per_hash_bucket;
  auto _num_evictor_partitions = _o->num_evictor_partitions;
  return homestorecfg::CreateCache(
      _fbb,
      _entries_per_hash_bucket,
      _num_evictor_partitions);
}

inline DeviceT *Device::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new DeviceT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Device::UnPackTo(DeviceT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = max_error_before_marking_dev_down(); _o->max_error_before_marking_dev_down = _e; };
  { auto _e = max_outstanding_ios_per_aio_thread(); _o->max_outstanding_ios_per_aio_thread = _e; };
  { auto _e = max_completions_process_per_event_per_thread(); _o->max_completions_process_per_event_per_thread = _e; };
}

inline flatbuffers::Offset<Device> Device::Pack(flatbuffers::FlatBufferBuilder &_fbb, const DeviceT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateDevice(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Device> CreateDevice(flatbuffers::FlatBufferBuilder &_fbb, const DeviceT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const DeviceT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _max_error_before_marking_dev_down = _o->max_error_before_marking_dev_down;
  auto _max_outstanding_ios_per_aio_thread = _o->max_outstanding_ios_per_aio_thread;
  auto _max_completions_process_per_event_per_thread = _o->max_completions_process_per_event_per_thread;
  return homestorecfg::CreateDevice(
      _fbb,
      _max_error_before_marking_dev_down,
      _max_outstanding_ios_per_aio_thread,
      _max_completions_process_per_event_per_thread);
}

inline GenericT *Generic::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new GenericT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void Generic::UnPackTo(GenericT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = cache_size_percent(); _o->cache_size_percent = _e; };
  { auto _e = soft_mem_release_threshold(); _o->soft_mem_release_threshold = _e; };
  { auto _e = aggressive_mem_release_threshold(); _o->aggressive_mem_release_threshold = _e; };
  { auto _e = mem_release_rate(); _o->mem_release_rate = _e; };
}

inline flatbuffers::Offset<Generic> Generic::Pack(flatbuffers::FlatBufferBuilder &_fbb, const GenericT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateGeneric(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<Generic> CreateGeneric(flatbuffers::FlatBufferBuilder &_fbb, const GenericT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const GenericT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _cache_size_percent = _o->cache_size_percent;
  auto _soft_mem_release_threshold = _o->soft_mem_release_threshold;
  auto _aggressive_mem_release_threshold = _o->aggressive_mem_release_threshold;
  auto _mem_release_rate = _o->mem_release_rate;
  return homestorecfg::CreateGeneric(
      _fbb,
      _cache_size_percent,
      _soft_mem_release_threshold,
      _aggressive_mem_release_threshold,
      _mem_release_rate);
}

inline HomeStoreSettingsT *HomeStoreSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new HomeStoreSettingsT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void HomeStoreSettings::UnPackTo(HomeStoreSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = version(); _o->version = _e; };
  { auto _e = generic(); if (_e) _o->generic = sisl::embedded_t<GenericT>(_e->UnPack(_resolver)); };
  { auto _e = blkallocator(); if (_e) _o->blkallocator = sisl::embedded_t<BlkAllocatorT>(_e->UnPack(_resolver)); };
  { auto _e = cache(); if (_e) _o->cache = sisl::embedded_t<CacheT>(_e->UnPack(_resolver)); };
  { auto _e = btree(); if (_e) _o->btree = sisl::embedded_t<BtreeT>(_e->UnPack(_resolver)); };
  { auto _e = device(); if (_e) _o->device = sisl::embedded_t<DeviceT>(_e->UnPack(_resolver)); };
}

inline flatbuffers::Offset<HomeStoreSettings> HomeStoreSettings::Pack(flatbuffers::FlatBufferBuilder &_fbb, const HomeStoreSettingsT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateHomeStoreSettings(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<HomeStoreSettings> CreateHomeStoreSettings(flatbuffers::FlatBufferBuilder &_fbb, const HomeStoreSettingsT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const HomeStoreSettingsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _version = _o->version;
  auto _generic = _o->generic ? CreateGeneric(_fbb, _o->generic.get(), _rehasher) : 0;
  auto _blkallocator = _o->blkallocator ? CreateBlkAllocator(_fbb, _o->blkallocator.get(), _rehasher) : 0;
  auto _cache = _o->cache ? CreateCache(_fbb, _o->cache.get(), _rehasher) : 0;
  auto _btree = _o->btree ? CreateBtree(_fbb, _o->btree.get(), _rehasher) : 0;
  auto _device = _o->device ? CreateDevice(_fbb, _o->device.get(), _rehasher) : 0;
  return homestorecfg::CreateHomeStoreSettings(
      _fbb,
      _version,
      _generic,
      _blkallocator,
      _cache,
      _btree,
      _device);
}

inline const flatbuffers::TypeTable *BlkAllocatorTypeTable() {
  static const flatbuffers::TypeCode type_codes[] = {
    { flatbuffers::ET_UINT, 0, -1 },
    { flatbuffers::ET_UINT, 0, -1 }
  };
  static const char * const names[] = {
    "max_cache_fill_varsize_blk_alloc_attempt",
    "max_varsize_blk_alloc_attempt"
  };
  static const flatbuffers::TypeTable tt = {
    flatbuffers::ST_TABLE, 2, type_codes, nullptr, nullptr, names
  };
  return &tt;
}

inline const flatbuffers::TypeTable *BtreeTypeTable() {
  static const flatbuffers::TypeCode type_codes[] = {
    { flatbuffers::ET_UINT, 0, -1 },
    { flatbuffers::ET_UINT, 0, -1 }
  };
  static const char * const names[] = {
    "max_nodes_to_rebalance",
    "mem_btree_page_size"
  };
  static const flatbuffers::TypeTable tt = {
    flatbuffers::ST_TABLE, 2, type_codes, nullptr, nullptr, names
  };
  return &tt;
}

inline const flatbuffers::TypeTable *CacheTypeTable() {
  static const flatbuffers::TypeCode type_codes[] = {
    { flatbuffers::ET_UINT, 0, -1 },
    { flatbuffers::ET_UINT, 0, -1 }
  };
  static const char * const names[] = {
    "entries_per_hash_bucket",
    "num_evictor_partitions"
  };
  static const flatbuffers::TypeTable tt = {
    flatbuffers::ST_TABLE, 2, type_codes, nullptr, nullptr, names
  };
  return &tt;
}

inline const flatbuffers::TypeTable *DeviceTypeTable() {
  static const flatbuffers::TypeCode type_codes[] = {
    { flatbuffers::ET_UINT, 0, -1 },
    { flatbuffers::ET_UINT, 0, -1 },
    { flatbuffers::ET_UINT, 0, -1 }
  };
  static const char * const names[] = {
    "max_error_before_marking_dev_down",
    "max_outstanding_ios_per_aio_thread",
    "max_completions_process_per_event_per_thread"
  };
  static const flatbuffers::TypeTable tt = {
    flatbuffers::ST_TABLE, 3, type_codes, nullptr, nullptr, names
  };
  return &tt;
}

inline const flatbuffers::TypeTable *GenericTypeTable() {
  static const flatbuffers::TypeCode type_codes[] = {
    { flatbuffers::ET_UINT, 0, -1 },
    { flatbuffers::ET_UINT, 0, -1 },
    { flatbuffers::ET_UINT, 0, -1 },
    { flatbuffers::ET_UINT, 0, -1 }
  };
  static const char * const names[] = {
    "cache_size_percent",
    "soft_mem_release_threshold",
    "aggressive_mem_release_threshold",
    "mem_release_rate"
  };
  static const flatbuffers::TypeTable tt = {
    flatbuffers::ST_TABLE, 4, type_codes, nullptr, nullptr, names
  };
  return &tt;
}

inline const flatbuffers::TypeTable *HomeStoreSettingsTypeTable() {
  static const flatbuffers::TypeCode type_codes[] = {
    { flatbuffers::ET_UINT, 0, -1 },
    { flatbuffers::ET_SEQUENCE, 0, 0 },
    { flatbuffers::ET_SEQUENCE, 0, 1 },
    { flatbuffers::ET_SEQUENCE, 0, 2 },
    { flatbuffers::ET_SEQUENCE, 0, 3 },
    { flatbuffers::ET_SEQUENCE, 0, 4 }
  };
  static const flatbuffers::TypeFunction type_refs[] = {
    GenericTypeTable,
    BlkAllocatorTypeTable,
    CacheTypeTable,
    BtreeTypeTable,
    DeviceTypeTable
  };
  static const char * const names[] = {
    "version",
    "generic",
    "blkallocator",
    "cache",
    "btree",
    "device"
  };
  static const flatbuffers::TypeTable tt = {
    flatbuffers::ST_TABLE, 6, type_codes, type_refs, nullptr, names
  };
  return &tt;
}

inline const homestorecfg::HomeStoreSettings *GetHomeStoreSettings(const void *buf) {
  return flatbuffers::GetRoot<homestorecfg::HomeStoreSettings>(buf);
}

inline const homestorecfg::HomeStoreSettings *GetSizePrefixedHomeStoreSettings(const void *buf) {
  return flatbuffers::GetSizePrefixedRoot<homestorecfg::HomeStoreSettings>(buf);
}

inline HomeStoreSettings *GetMutableHomeStoreSettings(void *buf) {
  return flatbuffers::GetMutableRoot<HomeStoreSettings>(buf);
}

inline bool VerifyHomeStoreSettingsBuffer(
    flatbuffers::Verifier &verifier) {
  return verifier.VerifyBuffer<homestorecfg::HomeStoreSettings>(nullptr);
}

inline bool VerifySizePrefixedHomeStoreSettingsBuffer(
    flatbuffers::Verifier &verifier) {
  return verifier.VerifySizePrefixedBuffer<homestorecfg::HomeStoreSettings>(nullptr);
}

inline void FinishHomeStoreSettingsBuffer(
    flatbuffers::FlatBufferBuilder &fbb,
    flatbuffers::Offset<homestorecfg::HomeStoreSettings> root) {
  fbb.Finish(root);
}

inline void FinishSizePrefixedHomeStoreSettingsBuffer(
    flatbuffers::FlatBufferBuilder &fbb,
    flatbuffers::Offset<homestorecfg::HomeStoreSettings> root) {
  fbb.FinishSizePrefixed(root);
}

inline sisl::embedded_t<HomeStoreSettingsT> UnPackHomeStoreSettings(
    const void *buf,
    const flatbuffers::resolver_function_t *res = nullptr) {
  return sisl::embedded_t<HomeStoreSettingsT>(GetHomeStoreSettings(buf)->UnPack(res));
}

}  // namespace homestorecfg

#endif  // FLATBUFFERS_GENERATED_HOMESTORECONFIG_HOMESTORECFG_H_
